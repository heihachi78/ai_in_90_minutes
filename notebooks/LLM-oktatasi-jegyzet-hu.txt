-------------------------------------------------
10. Beágyazások: Szemantikus hasonlóság
-------------------------------------------------
Cél:
- Szemantikus keresés, szövegek összehasonlítása

Mi az embedding?
- Szöveget számvektorokká alakítjuk, így hasonlóság mérhető

Példa:
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
def get_embedding(text):
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    return response.data[0].embedding
szovegek = [
    "A macska ül a szőnyegen.",
    "Egy cica pihen a padlón.",
    "Szeretek Pythont programozni.",
    "A Python a kedvenc nyelvem."
]
emb = [get_embedding(t) for t in szovegek]
sim = cosine_similarity(emb)
for i, a in enumerate(szovegek):
    for j, b in enumerate(szovegek):
        if i < j:
            print(f"'{a}' vs '{b}': {sim[i][j]:.3f}")

-------------------------------------------------
11. Bevezetés a RAG-hez (Retrieval-Augmented Generation)
-------------------------------------------------
Cél:
- A RAG (lekérdezés-alapú generálás) működésének bemutatása

Miért kell RAG?
- Az LLM-ek tudása korlátos (adott időpontig tanították őket)
- RAG: friss vagy céges adatok becsatolása a generáláshoz

RAG elv:
1. Lekérdezés (releváns dokumentumok keresése)
2. Kontextus bővítése (a legjobb találatok beillesztése a promptba)
3. Válaszgenerálás

Egyszerű példa:
dokumentumok = [
    "Azure OpenAI Service REST API elérést ad a nyelvi modellekhez.",
    "Embedding: szövegek numerikus vektorokká alakítása.",
    "A RAG ötvözi a keresést és a generálást.",
    "A funkcióhívás lehetővé teszi API-k használatát."
]
def simple_rag(query, docs, top_k=2):
    query_emb = get_embedding(query)
    sim_list = []
    for doc in docs:
        doc_emb = get_embedding(doc)
        similarity = cosine_similarity([query_emb], [doc_emb])[0][0]
        sim_list.append((doc, similarity))
    sim_list.sort(key=lambda x: x[1], reverse=True)
    relevans = [doc for doc, _ in sim_list[:top_k]]
    context = "\n".join(relevans)
    prompt = f"""Az alábbi információk alapján:\n{context}\nKérdés: {query}\nVálasz:"""
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content, relevans

kerdes = "Mi az Azure OpenAI Service?"
valasz, forrasok = simple_rag(kerdes, dokumentumok)
print(f"Kérdés: {kerdes}\nForrások: {forrasok}\nVálasz: {valasz}")

-------------------------------------------------
12. Haladó RAG koncepciók
-------------------------------------------------
Cél:
- Dokumentum darabolás (chunking), vektor-adatbázis, metaadat szűrés

Chunking:
- Hosszú dokumentumokat kis átfedő szakaszokra kell bontani

Vektor-adatbázis:
- Embeddingek hatékony tárolása, gyors keresés (pl. Azure Cognitive Search, Pinecone, Weaviate)

Haladó példa:
class HaladoRAG:
    def __init__(self):
        self.docs = [
            {"text": "Azure árak...", "kategoria": "ar", "datum": "2024-01-15"},
            {"text": "Azure technikai dokumentáció...", "kategoria": "technikai", "datum": "2024-02-01"},
        ]
    def lekerdezes_szurovel(self, query, kat=None):
        if kat:
            szurt = [d for d in self.docs if d["kategoria"] == kat]
        else:
            szurt = self.docs
        return szurt

rag = HaladoRAG()
print(rag.lekerdezes_szurovel("Mi az Azure ára?", kat="ar"))

-------------------------------------------------
13. Való életbeli alkalmazások és legjobb gyakorlatok
-------------------------------------------------
Leggyakoribb felhasználás:
- Dokumentum alapú kérdés-válasz rendszerek
- Kódasszisztensek, fejlesztői chatbotok
- Ügyfélszolgálati botok
- Tartalomgenerálás

Legjobb gyakorlatok:
- Hibakezelés, API limit figyelés, cache-elés, naplózás, biztonság (API-kulcsok védelme)
- Etikus használat, adatok védelme, torzítások elkerülése

-------------------------------------------------
14. Hibakeresés és problémamegoldás
-------------------------------------------------
Gyakori hibák:
- Token limit túllépés -> Kontextus rövidítése
- Gyenge keresés -> chunking, embedding javítása
- Lassú válasz -> kisebb kontextus, cache használat
- Irreleváns válasz -> prompt javítása, szűrés

Hibakereső kód:
def debug_rag_valasz(kerdes, talalatok, valasz):
    print("=== RAG debug ===")
    print(f"Kérdés: {kerdes}")
    print(f"Dokumentumok: {len(talalatok)} db")
    for i, d in enumerate(talalatok):
        print(f"Doc {i+1}: {d[:100]}...")
    print(f"Válasz hossza: {len(valasz)} karakter")
    print("=================")

-------------------------------------------------
15. Jövőbeli irányok és fejlett témák
-------------------------------------------------
- Multimodális LLM-ek (szöveg+képek)
- Ügynökök, automatizált eszközhasználat
- Finomhangolás céges/adott szakterületi adatokon

-------------------------------------------------
16. Gyakorló feladatok
-------------------------------------------------
1. Saját chatbot fejlesztése (FAQ alapján)
2. Dokumentum alapú Q&A rendszer (RAG)
3. Kódellenőrző asszisztens (kódminőség)
4. RAG teljesítmény optimalizálás (sebesség, pontosság)

-------------------------------------------------
17. További források
-------------------------------------------------
- Azure OpenAI dokumentáció: https://learn.microsoft.com/hu-hu/azure/cognitive-services/openai/
- OpenAI Cookbook: https://cookbook.openai.com/
- LangChain keretrendszer: https://python.langchain.com/
- Vektor-adatbázisok: https://www.pinecone.io/, https://weaviate.io/
- Üzembehelyezési útmutatók: https://learn.microsoft.com/hu-hu/azure/architecture/example-scenario/ai/openai-embeddings

-------------------------------------------------
18. Fogalomtár
-------------------------------------------------
- Token: Szöveg legkisebb egysége, amit az LLM értelmez
- Beágyazás (embedding): Szöveg numerikus vektor reprezentációja
- RAG: Retrieval-Augmented Generation (lekérdezés-alapú generálás)
- Kontextusablak: Maximálisan feldolgozható szöveghossz (tokenben)
- Temperature: Véletlenszerűség a kimenetben
- Prompt engineering: Hatékony utasítás írása az LLM számára
