import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import time

class FrozenLakeQLearning:
    def __init__(self, environment_name="FrozenLake-v1", is_slippery=False):
        """
        FrozenLake Q-Learning √°gens inicializ√°l√°sa
        
        Args:
            environment_name: A k√∂rnyezet neve
            is_slippery: Ha True, akkor a j√©g cs√∫sz√≥s (sztochasztikus k√∂rnyezet)
        """
        # K√∂rnyezet l√©trehoz√°sa
        #self.env = gym.make(environment_name, is_slippery=is_slippery, render_mode="grayscale")
        self.env = gym.make(environment_name, is_slippery=is_slippery, render_mode="human")
        
        # √Ållapot- √©s akci√≥t√©r m√©rete
        self.state_size = self.env.observation_space.n  # 16 (4x4 r√°cs)
        self.action_size = self.env.action_space.n      # 4 (fel, le, balra, jobbra)
        
        # Q-t√°bla inicializ√°l√°sa null√°kkal
        self.q_table = np.zeros((self.state_size, self.action_size))
        
        # Hiperparam√©terek
        self.learning_rate = 0.8    # Alpha - tanul√°si r√°ta
        self.discount_factor = 0.95 # Gamma - lesz√°m√≠tol√°si t√©nyez≈ë
        self.epsilon = 1.0          # Explor√°ci√≥ val√≥sz√≠n≈±s√©ge
        self.epsilon_min = 0.01     # Minimum explor√°ci√≥
        self.epsilon_decay = 0.995  # Explor√°ci√≥ cs√∂kken√©se
        
        # Statisztik√°k k√∂vet√©se
        self.rewards_per_episode = []
        self.epsilon_history = []
        
    def choose_action(self, state):
        """
        Epsilon-greedy strat√©gia: explor√°ci√≥ vs exploit√°ci√≥
        
        Args:
            state: Jelenlegi √°llapot
            
        Returns:
            action: V√°lasztott akci√≥ (0-3)
        """
        if np.random.random() < self.epsilon:
            # Explor√°ci√≥: v√©letlenszer≈± akci√≥
            return self.env.action_space.sample()
        else:
            # Exploit√°ci√≥: legjobb ismert akci√≥
            return np.argmax(self.q_table[state])
    
    def update_q_table(self, state, action, reward, next_state, done):
        """
        Q-t√°bla friss√≠t√©se a Bellman-egyenlet alapj√°n
        
        Q(s,a) = Q(s,a) + Œ± * [r + Œ≥ * max(Q(s',a')) - Q(s,a)]
        """
        # Jelenlegi Q-√©rt√©k
        current_q = self.q_table[state, action]
        
        if done:
            # Ha az epiz√≥d v√©get √©rt, nincs k√∂vetkez≈ë √°llapot
            target = reward
        else:
            # Bellman-egyenlet: jutalom + lesz√°m√≠tolt j√∂v≈ëbeli maximum Q-√©rt√©k
            target = reward + self.discount_factor * np.max(self.q_table[next_state])
        
        # Q-√©rt√©k friss√≠t√©se
        self.q_table[state, action] = current_q + self.learning_rate * (target - current_q)
    
    def decay_epsilon(self):
        """Explor√°ci√≥ cs√∂kkent√©se az id≈ë m√∫l√°s√°val"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def train(self, episodes=1000):
        """
        √Ågens tan√≠t√°sa Q-Learning algoritmussal
        
        Args:
            episodes: Tan√≠t√°si epiz√≥dok sz√°ma
        """
        print(f"Tan√≠t√°s kezd√©se {episodes} epiz√≥ddal...")
        print(f"K√∂rnyezet: {self.state_size} √°llapot, {self.action_size} akci√≥")
        print("-" * 50)
        
        for episode in range(episodes):
            # Epiz√≥d inicializ√°l√°sa
            state, _ = self.env.reset()
            total_reward = 0
            steps = 0
            done = False
            
            while not done:
                # Akci√≥ v√°laszt√°sa
                action = self.choose_action(state)
                
                # L√©p√©s a k√∂rnyezetben
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # Q-t√°bla friss√≠t√©se
                self.update_q_table(state, action, reward, next_state, done)
                
                # √Ållapot friss√≠t√©se
                state = next_state
                total_reward += reward
                steps += 1
            
            # Explor√°ci√≥ cs√∂kkent√©se
            self.decay_epsilon()
            
            # Statisztik√°k ment√©se
            self.rewards_per_episode.append(total_reward)
            self.epsilon_history.append(self.epsilon)
            
            # Halad√°s ki√≠r√°sa
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(self.rewards_per_episode[-100:])
                print(f"Epiz√≥d {episode + 1:4d} | "
                      f"√Åtlag jutalom (utols√≥ 100): {avg_reward:.3f} | "
                      f"Epsilon: {self.epsilon:.3f}")
        
        self.env.close()
        print("\nTan√≠t√°s befejezve!")
    
    def test(self, episodes=10, render=True):
        """
        Tan√≠tott √°gens tesztel√©se
        
        Args:
            episodes: Teszt epiz√≥dok sz√°ma
            render: Vizualiz√°ci√≥ megjelen√≠t√©se
        """
        if render:
            test_env = gym.make("FrozenLake-v1", is_slippery=False, render_mode="human")
        else:
            test_env = gym.make("FrozenLake-v1", is_slippery=False)
        
        print(f"\nTesztel√©s {episodes} epiz√≥ddal (explor√°ci√≥ kikapcsolva)...")
        successes = 0
        total_rewards = []
        
        for episode in range(episodes):
            state, _ = test_env.reset()
            total_reward = 0
            steps = 0
            done = False
            
            print(f"\nEpiz√≥d {episode + 1}:")
            
            while not done and steps < 100:  # Max 100 l√©p√©s
                # Mindig a legjobb akci√≥t v√°lasztjuk (epsilon = 0)
                action = np.argmax(self.q_table[state])
                next_state, reward, terminated, truncated, _ = test_env.step(action)
                done = terminated or truncated
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if render:
                    time.sleep(0.5)  # Lassabb megjelen√≠t√©s
            
            total_rewards.append(total_reward)
            if total_reward > 0:
                successes += 1
                print(f"‚úÖ Siker! {steps} l√©p√©sben el√©rte a c√©lt")
            else:
                print(f"‚ùå Kudarc {steps} l√©p√©s ut√°n")
        
        test_env.close()
        
        success_rate = successes / episodes * 100
        avg_reward = np.mean(total_rewards)
        
        print(f"\nüìä Teszt eredm√©nyek:")
        print(f"Sikeress√©gi ar√°ny: {success_rate:.1f}% ({successes}/{episodes})")
        print(f"√Åtlagos jutalom: {avg_reward:.3f}")
        
        return success_rate, avg_reward
    
    def plot_training_progress(self):
        """Tan√≠t√°si halad√°s vizualiz√°l√°sa"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Jutalmak alakul√°sa
        episodes = range(len(self.rewards_per_episode))
        ax1.plot(episodes, self.rewards_per_episode, alpha=0.6, color='blue')
        
        # Sim√≠tott g√∂rbe (100 epiz√≥dos √°tlag)
        if len(self.rewards_per_episode) >= 100:
            smoothed = [np.mean(self.rewards_per_episode[max(0, i-99):i+1]) 
                       for i in range(len(self.rewards_per_episode))]
            ax1.plot(episodes, smoothed, color='red', linewidth=2, label='100-epiz√≥d √°tlag')
            ax1.legend()
        
        ax1.set_title('Jutalmak alakul√°sa')
        ax1.set_xlabel('Epiz√≥d')
        ax1.set_ylabel('Jutalom')
        ax1.grid(True, alpha=0.3)
        
        # Epsilon alakul√°sa (explor√°ci√≥)
        ax2.plot(episodes, self.epsilon_history, color='green')
        ax2.set_title('Explor√°ci√≥ (epsilon) cs√∂kken√©se')
        ax2.set_xlabel('Epiz√≥d')
        ax2.set_ylabel('Epsilon √©rt√©k')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def print_q_table(self):
        """Q-t√°bla ki√≠r√°sa sz√©pen form√°zva"""
        print("\nüìã V√©gs≈ë Q-t√°bla:")
        print("   Akci√≥k:  Bal    Le   Jobb   Fel")
        print("-" * 40)
        for state in range(self.state_size):
            values = self.q_table[state]
            print(f"√Ållapot {state:2d}: {values[0]:6.3f} {values[1]:6.3f} {values[2]:6.3f} {values[3]:6.3f}")
    
    def print_policy(self):
        """Optim√°lis strat√©gia ki√≠r√°sa"""
        action_symbols = ['‚Üê', '‚Üì', '‚Üí', '‚Üë']
        print("\nüéØ Tanult strat√©gia (optim√°lis akci√≥k):")
        
        policy = np.argmax(self.q_table, axis=1)
        
        # 4x4 r√°csk√©nt megjelen√≠t√©s
        for row in range(4):
            row_str = ""
            for col in range(4):
                state = row * 4 + col
                if state in [5, 7, 11, 12]:  # Lyukak (√°ltal√°ban)
                    row_str += " H "
                elif state == 15:  # C√©l
                    row_str += " G "
                elif state == 0:   # Start
                    row_str += " S "
                else:
                    row_str += f" {action_symbols[policy[state]]} "
            print(row_str)

# Haszn√°lat p√©lda
if __name__ == "__main__":
    # √Ågens l√©trehoz√°sa
    agent = FrozenLakeQLearning(is_slippery=False)  # Determinisztikus verzi√≥
    
    print("üéÆ FrozenLake Q-Learning megold√≥")
    print("=" * 50)
    print("C√©l: Eljutni az S-t≈ël a G-ig a lyukak (H) elker√ºl√©s√©vel")
    print("Akci√≥k: 0=Bal, 1=Le, 2=Jobb, 3=Fel")
    print()
    
    # Tan√≠t√°s
    agent.train(episodes=1000)
    
    # Eredm√©nyek megjelen√≠t√©se
    agent.plot_training_progress()
    agent.print_policy()
    agent.print_q_table()
    
    # Tesztel√©s
    agent.test(episodes=5, render=True)